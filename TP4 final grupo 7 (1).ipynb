{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3296e8a-abdc-4186-83fa-be84500f5248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos Combinados:\n",
      "                           CODUSU  ANO4_x  TRIMESTRE_x  NRO_HOGAR  COMPONENTE  \\\n",
      "0  TQRMNOTUUHJMLPCDEIIAD00801670    2023            4          1           1   \n",
      "1  TQRMNOTUUHJMLPCDEIIAD00801670    2023            4          1           2   \n",
      "2  TQRMNOPUUHJKLQCDEIIAD00793187    2023            4          1           1   \n",
      "3  TQRMNOPUUHJKLQCDEIIAD00793187    2023            4          1           2   \n",
      "4  TQRMNOPUTHKLMNCDEIIAD00791268    2023            4          1           1   \n",
      "\n",
      "   H15  REGION_x MAS_500_x  AGLOMERADO_x  PONDERA_x  ...  GDECCFR_y  \\\n",
      "0    1         1         S            32       1775  ...        8.0   \n",
      "1    1         1         S            32       1775  ...        8.0   \n",
      "2    2         1         S            32        609  ...       12.0   \n",
      "3    1         1         S            32        609  ...       12.0   \n",
      "4    1         1         S            32       3129  ...        8.0   \n",
      "\n",
      "   PDECCFR_y ADECCFR_y  PONDIH_y  VII1_1  VII1_2  VII2_1  VII2_2  VII2_3  \\\n",
      "0        NaN         5      3079       2       0      98       0       0   \n",
      "1        NaN         5      3079       2       0      98       0       0   \n",
      "2        NaN        12         0       2       0      98       0       0   \n",
      "3        NaN        12         0       2       0      98       0       0   \n",
      "4        NaN         6      4648       1       0      98       0       0   \n",
      "\n",
      "   VII2_4  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "\n",
      "[5 rows x 263 columns]\n"
     ]
    }
   ],
   "source": [
    "#PARTE 1\n",
    "#INCISO 2\n",
    "import pandas as pd\n",
    "\n",
    "# Rutas de los archivos\n",
    "ruta_hogar = 'C:/Users/User/Downloads/Big data - TP2/TP3/usu_hogar_T423.xlsx'\n",
    "ruta_individual = 'C:/Users/User/Downloads/Big data - TP2/TP3/usu_individual_T423.xlsx'\n",
    "\n",
    "\n",
    "# Lectura de los archivos de Excel\n",
    "datos_hogar = pd.read_excel(ruta_hogar)\n",
    "datos_individual = pd.read_excel(ruta_individual)\n",
    "\n",
    "# Renombrar columnas para evitar conflictos en el merge\n",
    "datos_individual.rename(columns={'ITF': 'ITF_persona', 'IPCF': 'IPCF_persona'}, inplace=True)\n",
    "\n",
    "# Filtrar datos para GBA (REGION == 1)\n",
    "gba_hogar = datos_hogar[datos_hogar['REGION'] == 1]\n",
    "gba_individual = datos_individual[datos_individual['REGION'] == 1]\n",
    "\n",
    "# Fusionar los DataFrames filtrados\n",
    "datos_combinados = pd.merge(gba_individual, gba_hogar, on=['CODUSU', 'NRO_HOGAR'])\n",
    "\n",
    "# Mostrar primeras filas del DataFrame combinado\n",
    "print(\"Datos Combinados:\\n\", datos_combinados.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d51b51-51b1-4cea-a09e-c8b31106e264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Limpio:\n",
      "                           CODUSU  ANO4_x  TRIMESTRE_x  NRO_HOGAR  COMPONENTE  \\\n",
      "0   TQRMNOTUUHJMLPCDEIIAD00801670    2023            4          1           1   \n",
      "4   TQRMNOPUTHKLMNCDEIIAD00791268    2023            4          1           1   \n",
      "5   TQRMNOPUTHKLMNCDEIIAD00791268    2023            4          1           2   \n",
      "7   TQRMNORXPHKLMNCDEIIAD00791273    2023            4          2           1   \n",
      "11  TQRMNOQWYHLMKRCDEIJAH00854987    2023            4          1           1   \n",
      "\n",
      "    H15  REGION_x MAS_500_x  AGLOMERADO_x  PONDERA_x  ...  GDECCFR_y  \\\n",
      "0     1         1         S            32       1775  ...        8.0   \n",
      "4     1         1         S            32       3129  ...        8.0   \n",
      "5     1         1         S            32       3129  ...        8.0   \n",
      "7     1         1         S            32       3357  ...        8.0   \n",
      "11    1         1         S            33       1656  ...        4.0   \n",
      "\n",
      "    PDECCFR_y ADECCFR_y  PONDIH_y  VII1_1  VII1_2  VII2_1  VII2_2  VII2_3  \\\n",
      "0         NaN         5      3079       2       0      98       0       0   \n",
      "4         NaN         6      4648       1       0      98       0       0   \n",
      "5         NaN         6      4648       1       0      98       0       0   \n",
      "7         NaN         5      3050       1       0      98       0       0   \n",
      "11        NaN         4      2575       2       0      98       0       0   \n",
      "\n",
      "    VII2_4  \n",
      "0        0  \n",
      "4        0  \n",
      "5        0  \n",
      "7        0  \n",
      "11       0  \n",
      "\n",
      "[5 rows x 258 columns]\n"
     ]
    }
   ],
   "source": [
    "#INCISO 3:\n",
    "# Filtrar variables binarias para valores válidos (1 o 2)\n",
    "variables_binarias = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19_A', 'V19_B', 'V21', 'V22']\n",
    "for var in variables_binarias:\n",
    "    if var in datos_combinados.columns:\n",
    "        datos_combinados = datos_combinados[datos_combinados[var].isin([1, 2])]\n",
    "\n",
    "# Eliminar columnas que no aportan valor\n",
    "columnas_eliminar = ['IV1_ESP', 'IV3_ESP', 'IV7_ESP', 'II7_ESP', 'II8_ESP']\n",
    "datos_combinados.drop(columns=[col for col in columnas_eliminar if col in datos_combinados.columns], inplace=True)\n",
    "\n",
    "# Filtrar ingresos, edad, horas trabajadas, cantidad de habitantes en el hogar y habitaciones\n",
    "if 'IPCF' in datos_combinados.columns and 'ITF_persona' in datos_combinados.columns:\n",
    "    datos_combinados = datos_combinados[(datos_combinados['IPCF'] >= 0) & (datos_combinados['ITF_persona'] >= 0)]\n",
    "if 'CH06' in datos_combinados.columns:\n",
    "    datos_combinados = datos_combinados[datos_combinados['CH06'] >= 0]\n",
    "if 'PP3E_TOT' in datos_combinados.columns:\n",
    "    datos_combinados = datos_combinados[(datos_combinados['PP3E_TOT'] >= 0) & (datos_combinados['PP3E_TOT'] <= 168)]\n",
    "if 'PP3F_TOT' in datos_combinados.columns:\n",
    "    datos_combinados = datos_combinados[(datos_combinados['PP3F_TOT'] >= 0) & (datos_combinados['PP3F_TOT'] <= 168)]\n",
    "if 'IX_TOT' in datos_combinados.columns and 'IX_MEN10' in datos_combinados.columns and 'IX_MAYEQ10' in datos_combinados.columns:\n",
    "    datos_combinados = datos_combinados[(datos_combinados['IX_TOT'] >= 0) & (datos_combinados['IX_MEN10'] >= 0) & (datos_combinados['IX_MAYEQ10'] >= 0)]\n",
    "if 'II1' in datos_combinados.columns and 'II2' in datos_combinados.columns:\n",
    "    datos_combinados = datos_combinados[(datos_combinados['II1'] >= 0) & (datos_combinados['II2'] >= 0) & (datos_combinados['II2'] <= datos_combinados['II1'])]\n",
    "if 'II3_1' in datos_combinados.columns and 'II3' in datos_combinados.columns:\n",
    "    datos_combinados = datos_combinados[(datos_combinados['II3_1'] >= 0) & (datos_combinados['II3_1'] <= datos_combinados['II3'])]\n",
    "\n",
    "# Mostrar el DataFrame después de filtrar los valores sin sentido\n",
    "print(\"\\nDataFrame Limpio:\")\n",
    "print(datos_combinados.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0afde6c6-ddbf-4d21-b758-9ba5748c1888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estadísticas Descriptivas:\n",
      "       habitantes_por_cuarto  ingreso_por_adulto          IPCF     PP3E_TOT  \\\n",
      "count            3443.000000        3.443000e+03  3.443000e+03  3443.000000   \n",
      "mean                     inf        1.453690e+05  1.314657e+05    36.579437   \n",
      "std                      NaN        2.101405e+05  1.960944e+05    17.548398   \n",
      "min                 0.333333        0.000000e+00  0.000000e+00     0.000000   \n",
      "25%                 1.333333        0.000000e+00  0.000000e+00    24.000000   \n",
      "50%                 1.500000        8.666667e+04  7.333333e+04    40.000000   \n",
      "75%                 2.000000        2.150667e+05  2.000000e+05    48.000000   \n",
      "max                      inf        2.100000e+06  2.100000e+06   133.000000   \n",
      "\n",
      "       asistencia_familiar  \n",
      "count          3443.000000  \n",
      "mean           4008.039791  \n",
      "std           15738.100568  \n",
      "min              -9.000000  \n",
      "25%               0.000000  \n",
      "50%               0.000000  \n",
      "75%               0.000000  \n",
      "max          166000.000000  \n"
     ]
    }
   ],
   "source": [
    "#INCISO 4 y 5 :\n",
    "# Crear nuevas variables relevantes para predecir pobreza\n",
    "datos_combinados['habitantes_por_cuarto'] = datos_combinados['IX_TOT'] / datos_combinados['II2']\n",
    "datos_combinados['ingreso_por_adulto'] = datos_combinados['ITF_persona'] / datos_combinados['IX_MAYEQ10']\n",
    "datos_combinados['asistencia_familiar'] = datos_combinados.groupby(['CODUSU', 'NRO_HOGAR'])['V5_M'].transform('sum')\n",
    "\n",
    "# Seleccionar variables relevantes para el análisis\n",
    "variables_relevantes = ['habitantes_por_cuarto', 'ingreso_por_adulto', 'IPCF', 'PP3E_TOT', 'asistencia_familiar']\n",
    "\n",
    "# Calcular estadísticas descriptivas\n",
    "estadisticas_descriptivas = datos_combinados[variables_relevantes].describe()\n",
    "\n",
    "# Mostrar estadísticas descriptivas\n",
    "print(\"\\nEstadísticas Descriptivas:\")\n",
    "print(estadisticas_descriptivas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22972a33-df27-4a95-82f7-6452c0115f6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datos_combinados' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Seleccionar las variables para el gráfico\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m x_variable \u001b[38;5;241m=\u001b[39m datos_combinados[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mITF_persona\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m y_variable \u001b[38;5;241m=\u001b[39m datos_combinados[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV5\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Crear un gráfico de dispersión\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datos_combinados' is not defined"
     ]
    }
   ],
   "source": [
    "#INCISO 6:\n",
    "import matplotlib.pyplot as plt\n",
    "# Seleccionar las variables para el gráfico\n",
    "x_variable = datos_combinados['ITF_persona']\n",
    "y_variable = datos_combinados['V5']\n",
    "\n",
    "# Crear un gráfico de dispersión\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_variable, y_variable, alpha=0.5)\n",
    "plt.xlabel('Ingreso Total Familiar (ITF)')\n",
    "plt.ylabel('Recepción de Ayuda Social (V5)')\n",
    "plt.title('Relación entre ITF y Recepción de Ayuda Social (V5)')\n",
    "\n",
    "# Ajustar el rango del eje x\n",
    "plt.xlim(left=0, right=5000000)\n",
    "\n",
    "# Añadir grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n",
    "\n",
    "# Añadir comentario al gráfico\n",
    "plt.text(50, 1.5,\n",
    "         \"Observamos que los hogares con mayores niveles de ITF tienden a no recibir ayudas sociales.\",\n",
    "         fontsize=10, style='italic',\n",
    "         bbox={'facecolor': 'lightblue', 'alpha': 0.5, 'pad': 10})\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9d7f6c-918e-4212-b577-8e4c399bdbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CODUSU', 'ANO4', 'TRIMESTRE', 'NRO_HOGAR', 'COMPONENTE', 'H15',\n",
      "       'REGION', 'MAS_500', 'AGLOMERADO', 'PONDERA',\n",
      "       ...\n",
      "       'DECCFR', 'IDECCFR', 'RDECCFR', 'GDECCFR', 'PDECCFR', 'ADECCFR',\n",
      "       'PONDIH', 'Edad', 'adulto_equiv', 'ad_equiv_hogar'],\n",
      "      dtype='object', length=180)\n",
      "                           CODUSU  NRO_HOGAR  CH04  CH06     Edad  \\\n",
      "0   TQRMNOTUUHJMLPCDEIIAD00801670          1     1    76  76 años   \n",
      "1   TQRMNOTUUHJMLPCDEIIAD00801670          1     2    79  79 años   \n",
      "2   TQRMNOPUUHJKLQCDEIIAD00793187          1     1    65  65 años   \n",
      "3   TQRMNOPUUHJKLQCDEIIAD00793187          1     2    66  66 años   \n",
      "4   TQRMNOPUTHKLMNCDEIIAD00791268          1     2    49  49 años   \n",
      "5   TQRMNOPUTHKLMNCDEIIAD00791268          1     1    20  20 años   \n",
      "6   TQRMNOPUTHKLMNCDEIIAD00791268          1     1    17  17 años   \n",
      "7   TQRMNORXPHKLMNCDEIIAD00791273          2     1    39  39 años   \n",
      "8   TQRMNORXPHKLMNCDEIIAD00791273          2     1     5   5 años   \n",
      "9   TQRMNOSPTHKKMPCDEIIAD00791441          1     1    89  89 años   \n",
      "10  TQRMNOSPTHKKMPCDEIIAD00791441          1     1    72  72 años   \n",
      "11  TQRMNOQWYHLMKRCDEIJAH00854987          1     1    57  57 años   \n",
      "12  TQRMNOQWYHLMKRCDEIJAH00854987          1     2    51  51 años   \n",
      "13  TQRMNOPWXHJMLQCDEIJAH00794277          1     1    55  55 años   \n",
      "14  TQRMNOPWXHJMLQCDEIJAH00794277          1     2    51  51 años   \n",
      "15  TQRMNOPWXHJMLQCDEIJAH00794277          1     2    15  15 años   \n",
      "16  TQRMNOPWXHJMLQCDEIJAH00794277          1     2    12  12 años   \n",
      "17  TQRMNOPWXHJMLQCDEIJAH00794277          1     2    27  27 años   \n",
      "18  TQRMNOPWXHJMLQCDEIJAH00794277          1     2     1   1 años   \n",
      "19  TQRMNOPRWHKMLMCDEIJAH00794278          1     2    24  24 años   \n",
      "\n",
      "    adulto_equiv  ad_equiv_hogar  \n",
      "0           1.00            2.00  \n",
      "1           1.00            2.00  \n",
      "2           1.00            2.00  \n",
      "3           1.00            2.00  \n",
      "4           1.00            3.04  \n",
      "5           1.00            3.04  \n",
      "6           1.04            3.04  \n",
      "7           1.00            1.60  \n",
      "8           0.60            1.60  \n",
      "9           1.00            2.00  \n",
      "10          1.00            2.00  \n",
      "11          1.00            2.00  \n",
      "12          1.00            2.00  \n",
      "13          1.00            5.01  \n",
      "14          1.00            5.01  \n",
      "15          0.77            5.01  \n",
      "16          0.74            5.01  \n",
      "17          1.00            5.01  \n",
      "18          0.50            5.01  \n",
      "19          1.00            4.00  \n"
     ]
    }
   ],
   "source": [
    "#INCISO 7:\n",
    "#Volvemos a correr ejercicios del TP3.\n",
    "import pandas as pd\n",
    "\n",
    "# Leer los archivos Excel\n",
    "usu_individual = pd.read_excel('C:/Users/User/Downloads/Big data - TP2/TP3/usu_individual_T423.xlsx')\n",
    "tabla_adulto_equiv = pd.read_excel('C:/Users/User/Downloads/Big data - TP2/TP3/tabla_adulto_equiv2.xlsx')\n",
    "\n",
    "# Eliminar los espacios adicionales en los nombres de las columnas\n",
    "usu_individual.columns = usu_individual.columns.str.strip()\n",
    "\n",
    "# Crear un diccionario para mapear los valores de adulto equivalente\n",
    "equivalencias = {\n",
    "    (row['Edad'], 'Mujeres'): row['Mujeres'] for idx, row in tabla_adulto_equiv.iterrows()\n",
    "}\n",
    "equivalencias.update({\n",
    "    (row['Edad'], 'Varones'): row['Varones'] for idx, row in tabla_adulto_equiv.iterrows()\n",
    "})\n",
    "\n",
    "# Convertir la columna de edad en `usu_individual` para coincidir con el formato de `tabla_adulto_equiv`\n",
    "usu_individual['Edad'] = usu_individual['CH06'].apply(lambda x: f\"{x} años\" if x > 0 else \"Menor de 1 año\")\n",
    "\n",
    "# Asignar el valor de adulto equivalente con valores por defecto\n",
    "def obtener_equivalente(row):\n",
    "    if (row['Edad'], 'Mujeres' if row['CH04'] == 2 else 'Varones') in equivalencias:\n",
    "        return equivalencias[(row['Edad'], 'Mujeres' if row['CH04'] == 2 else 'Varones')]\n",
    "    else:\n",
    "        return 1.0 if row['CH06'] > 18 else 0.5\n",
    "\n",
    "usu_individual['adulto_equiv'] = usu_individual.apply(obtener_equivalente, axis=1)\n",
    "\n",
    "# Sumar los valores de adulto_equiv para cada hogar\n",
    "ad_equiv_hogar = usu_individual.groupby(['CODUSU', 'NRO_HOGAR'])['adulto_equiv'].sum().reset_index()\n",
    "\n",
    "# Renombrar la columna\n",
    "ad_equiv_hogar.rename(columns={'adulto_equiv': 'ad_equiv_hogar'}, inplace=True)\n",
    "\n",
    "# Unir esta nueva columna con la tabla original `usu_individual`\n",
    "usu_individual = usu_individual.merge(ad_equiv_hogar, on=['CODUSU', 'NRO_HOGAR'], how='left')\n",
    "\n",
    "# Verificar los nombres de las columnas después de la unión\n",
    "print(usu_individual.columns)\n",
    "\n",
    "# Limpiar la duplicación de columnas si es necesario\n",
    "if 'ad_equiv_hogar_x' in usu_individual.columns and 'ad_equiv_hogar_y' in usu_individual.columns:\n",
    "    usu_individual = usu_individual.drop(columns=['ad_equiv_hogar_x']).rename(columns={'ad_equiv_hogar_y': 'ad_equiv_hogar'})\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame resultante\n",
    "print(usu_individual[['CODUSU', 'NRO_HOGAR', 'CH04', 'CH06', 'Edad', 'adulto_equiv', 'ad_equiv_hogar']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c4bf759-b763-4635-8e91-563bb64a8df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de personas que no respondieron su ITF: 2979\n",
      "Cantidad de personas que respondieron su ITF: 4401\n"
     ]
    }
   ],
   "source": [
    "# 3.\n",
    "# Cuantas personas no respondieron cuál es su ingreso total familiar (ITF)\n",
    "norespondieron = usu_individual[usu_individual['ITF'].isna() | (usu_individual['ITF'] == 0)]\n",
    "respondieron = usu_individual[~usu_individual['ITF'].isna() & (usu_individual['ITF'] != 0)]\n",
    "\n",
    "# Guardar las bases distintas\n",
    "norespondieron.to_csv('norespondieron.csv', index=False)\n",
    "respondieron.to_csv('respondieron.csv', index=False)\n",
    "\n",
    "# Mostrar el número de personas que no respondieron\n",
    "print(f\"Cantidad de personas que no respondieron su ITF: {len(norespondieron)}\")\n",
    "print(f\"Cantidad de personas que respondieron su ITF: {len(respondieron)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17080ce3-6f26-49d2-b424-99a4e047e952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           CODUSU  NRO_HOGAR  ad_equiv_hogar  \\\n",
      "0   TQRMNOTUUHJMLPCDEIIAD00801670          1            2.00   \n",
      "1   TQRMNOTUUHJMLPCDEIIAD00801670          1            2.00   \n",
      "2   TQRMNOPUTHKLMNCDEIIAD00791268          1            3.04   \n",
      "3   TQRMNOPUTHKLMNCDEIIAD00791268          1            3.04   \n",
      "4   TQRMNOPUTHKLMNCDEIIAD00791268          1            3.04   \n",
      "5   TQRMNORXPHKLMNCDEIIAD00791273          2            1.60   \n",
      "6   TQRMNORXPHKLMNCDEIIAD00791273          2            1.60   \n",
      "7   TQRMNOSPTHKKMPCDEIIAD00791441          1            2.00   \n",
      "8   TQRMNOSPTHKKMPCDEIIAD00791441          1            2.00   \n",
      "9   TQRMNOQWYHLMKRCDEIJAH00854987          1            2.00   \n",
      "10  TQRMNOQWYHLMKRCDEIJAH00854987          1            2.00   \n",
      "11  TQRMNOPWXHJMLQCDEIJAH00794277          1            5.01   \n",
      "12  TQRMNOPWXHJMLQCDEIJAH00794277          1            5.01   \n",
      "13  TQRMNOPWXHJMLQCDEIJAH00794277          1            5.01   \n",
      "14  TQRMNOPWXHJMLQCDEIJAH00794277          1            5.01   \n",
      "15  TQRMNOPWXHJMLQCDEIJAH00794277          1            5.01   \n",
      "16  TQRMNOPWXHJMLQCDEIJAH00794277          1            5.01   \n",
      "17  TQRMNOTWVHMNNRCDEIJAH00812603          1            2.00   \n",
      "18  TQRMNOTWVHMNNRCDEIJAH00812603          1            2.00   \n",
      "19  TQRMNORYVHMNMLCDEIJAH00854984          1            3.00   \n",
      "\n",
      "    ingreso_necesario  \n",
      "0          265706.600  \n",
      "1          265706.600  \n",
      "2          403874.032  \n",
      "3          403874.032  \n",
      "4          403874.032  \n",
      "5          212565.280  \n",
      "6          212565.280  \n",
      "7          265706.600  \n",
      "8          265706.600  \n",
      "9          265706.600  \n",
      "10         265706.600  \n",
      "11         665595.033  \n",
      "12         665595.033  \n",
      "13         665595.033  \n",
      "14         665595.033  \n",
      "15         665595.033  \n",
      "16         665595.033  \n",
      "17         265706.600  \n",
      "18         265706.600  \n",
      "19         398559.900  \n"
     ]
    }
   ],
   "source": [
    "# 4. \n",
    "import pandas as pd\n",
    "\n",
    "# Cargar la base respondieron\n",
    "respondieron = pd.read_csv('respondieron.csv', low_memory=False)\n",
    "\n",
    "# Definir el valor de la Canasta Básica Total para un adulto equivalente\n",
    "canasta_basica_total = 132853.3\n",
    "\n",
    "# Calcular el ingreso necesario para cada hogar\n",
    "respondieron['ingreso_necesario'] = respondieron['ad_equiv_hogar'] * canasta_basica_total\n",
    "\n",
    "# Guardar la base actualizada\n",
    "respondieron.to_csv('respondieron_actualizado.csv', index=False)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame resultante para verificar\n",
    "print(respondieron[['CODUSU', 'NRO_HOGAR', 'ad_equiv_hogar', 'ingreso_necesario']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4287e02-7112-43fb-a3f1-ee503b2d89bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           CODUSU  NRO_HOGAR  CH04  CH06     Edad  \\\n",
      "0   TQRMNOTUUHJMLPCDEIIAD00801670          1     1    76  76 años   \n",
      "1   TQRMNOTUUHJMLPCDEIIAD00801670          1     2    79  79 años   \n",
      "2   TQRMNOPUUHJKLQCDEIIAD00793187          1     1    65  65 años   \n",
      "3   TQRMNOPUUHJKLQCDEIIAD00793187          1     2    66  66 años   \n",
      "4   TQRMNOPUTHKLMNCDEIIAD00791268          1     2    49  49 años   \n",
      "5   TQRMNOPUTHKLMNCDEIIAD00791268          1     1    20  20 años   \n",
      "6   TQRMNOPUTHKLMNCDEIIAD00791268          1     1    17  17 años   \n",
      "7   TQRMNORXPHKLMNCDEIIAD00791273          2     1    39  39 años   \n",
      "8   TQRMNORXPHKLMNCDEIIAD00791273          2     1     5   5 años   \n",
      "9   TQRMNOSPTHKKMPCDEIIAD00791441          1     1    89  89 años   \n",
      "10  TQRMNOSPTHKKMPCDEIIAD00791441          1     1    72  72 años   \n",
      "11  TQRMNOQWYHLMKRCDEIJAH00854987          1     1    57  57 años   \n",
      "12  TQRMNOQWYHLMKRCDEIJAH00854987          1     2    51  51 años   \n",
      "13  TQRMNOPWXHJMLQCDEIJAH00794277          1     1    55  55 años   \n",
      "14  TQRMNOPWXHJMLQCDEIJAH00794277          1     2    51  51 años   \n",
      "15  TQRMNOPWXHJMLQCDEIJAH00794277          1     2    15  15 años   \n",
      "16  TQRMNOPWXHJMLQCDEIJAH00794277          1     2    12  12 años   \n",
      "17  TQRMNOPWXHJMLQCDEIJAH00794277          1     2    27  27 años   \n",
      "18  TQRMNOPWXHJMLQCDEIJAH00794277          1     2     1   1 años   \n",
      "19  TQRMNOPRWHKMLMCDEIJAH00794278          1     2    24  24 años   \n",
      "\n",
      "    adulto_equiv  ad_equiv_hogar  \n",
      "0           1.00            2.00  \n",
      "1           1.00            2.00  \n",
      "2           1.00            2.00  \n",
      "3           1.00            2.00  \n",
      "4           1.00            3.04  \n",
      "5           1.00            3.04  \n",
      "6           1.04            3.04  \n",
      "7           1.00            1.60  \n",
      "8           0.60            1.60  \n",
      "9           1.00            2.00  \n",
      "10          1.00            2.00  \n",
      "11          1.00            2.00  \n",
      "12          1.00            2.00  \n",
      "13          1.00            5.01  \n",
      "14          1.00            5.01  \n",
      "15          0.77            5.01  \n",
      "16          0.74            5.01  \n",
      "17          1.00            5.01  \n",
      "18          0.50            5.01  \n",
      "19          1.00            4.00  \n",
      "Cantidad de personas que no respondieron su ITF: 2979\n",
      "Cantidad de personas que respondieron su ITF: 4401\n",
      "                           CODUSU  NRO_HOGAR     ITF  ad_equiv_hogar  \\\n",
      "0   TQRMNOTUUHJMLPCDEIIAD00801670          1  450000            2.00   \n",
      "1   TQRMNOTUUHJMLPCDEIIAD00801670          1  450000            2.00   \n",
      "4   TQRMNOPUTHKLMNCDEIIAD00791268          1  800000            3.04   \n",
      "5   TQRMNOPUTHKLMNCDEIIAD00791268          1  800000            3.04   \n",
      "6   TQRMNOPUTHKLMNCDEIIAD00791268          1  800000            3.04   \n",
      "7   TQRMNORXPHKLMNCDEIIAD00791273          2  420000            1.60   \n",
      "8   TQRMNORXPHKLMNCDEIIAD00791273          2  420000            1.60   \n",
      "9   TQRMNOSPTHKKMPCDEIIAD00791441          1  285000            2.00   \n",
      "10  TQRMNOSPTHKKMPCDEIIAD00791441          1  285000            2.00   \n",
      "11  TQRMNOQWYHLMKRCDEIJAH00854987          1  200000            2.00   \n",
      "12  TQRMNOQWYHLMKRCDEIJAH00854987          1  200000            2.00   \n",
      "13  TQRMNOPWXHJMLQCDEIJAH00794277          1  575000            5.01   \n",
      "14  TQRMNOPWXHJMLQCDEIJAH00794277          1  575000            5.01   \n",
      "15  TQRMNOPWXHJMLQCDEIJAH00794277          1  575000            5.01   \n",
      "16  TQRMNOPWXHJMLQCDEIJAH00794277          1  575000            5.01   \n",
      "17  TQRMNOPWXHJMLQCDEIJAH00794277          1  575000            5.01   \n",
      "18  TQRMNOPWXHJMLQCDEIJAH00794277          1  575000            5.01   \n",
      "22  TQRMNOTWVHMNNRCDEIJAH00812603          1  215000            2.00   \n",
      "23  TQRMNOTWVHMNNRCDEIJAH00812603          1  215000            2.00   \n",
      "24  TQRMNORYVHMNMLCDEIJAH00854984          1  550000            3.00   \n",
      "\n",
      "    ingreso_necesario  pobre  \n",
      "0          265706.600      0  \n",
      "1          265706.600      0  \n",
      "4          403874.032      0  \n",
      "5          403874.032      0  \n",
      "6          403874.032      0  \n",
      "7          212565.280      0  \n",
      "8          212565.280      0  \n",
      "9          265706.600      0  \n",
      "10         265706.600      0  \n",
      "11         265706.600      1  \n",
      "12         265706.600      1  \n",
      "13         665595.033      1  \n",
      "14         665595.033      1  \n",
      "15         665595.033      1  \n",
      "16         665595.033      1  \n",
      "17         665595.033      1  \n",
      "18         665595.033      1  \n",
      "22         265706.600      1  \n",
      "23         265706.600      1  \n",
      "24         398559.900      0  \n",
      "Total de hogares pobres: 2176\n",
      "Total de hogares: 4401\n",
      "Proporción de hogares pobres: 49.44%\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'PONDIH'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'PONDIH'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m hogares_completos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mes_pobre\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m hogares_completos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpobre\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Calcular la tasa de pobreza ponderada\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m total_hogares_ponderado \u001b[38;5;241m=\u001b[39m hogares_completos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPONDIH\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     98\u001b[0m total_hogares_pobres_ponderado \u001b[38;5;241m=\u001b[39m hogares_completos[hogares_completos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mes_pobre\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPONDIH\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     99\u001b[0m tasa_pobreza_ponderada \u001b[38;5;241m=\u001b[39m total_hogares_pobres_ponderado \u001b[38;5;241m/\u001b[39m total_hogares_ponderado\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'PONDIH'"
     ]
    }
   ],
   "source": [
    "#INCISO 8 Y 9\n",
    "import pandas as pd\n",
    "\n",
    "# Leer los archivos Excel\n",
    "usu_individual = pd.read_excel('C:/Users/User/Downloads/Big data - TP2/TP3/usu_individual_T423.xlsx')\n",
    "tabla_adulto_equiv = pd.read_excel('C:/Users/User/Downloads/Big data - TP2/TP3/tabla_adulto_equiv2.xlsx')\n",
    "\n",
    "# Eliminar los espacios adicionales en los nombres de las columnas\n",
    "usu_individual.columns = usu_individual.columns.str.strip()\n",
    "\n",
    "# Crear un diccionario para mapear los valores de adulto equivalente\n",
    "equivalencias = {\n",
    "    (row['Edad'], 'Mujeres'): row['Mujeres'] for idx, row in tabla_adulto_equiv.iterrows()\n",
    "}\n",
    "equivalencias.update({\n",
    "    (row['Edad'], 'Varones'): row['Varones'] for idx, row in tabla_adulto_equiv.iterrows()\n",
    "})\n",
    "\n",
    "# Convertir la columna de edad en `usu_individual` para coincidir con el formato de `tabla_adulto_equiv`\n",
    "usu_individual['Edad'] = usu_individual['CH06'].apply(lambda x: f\"{x} años\" if x > 0 else \"Menor de 1 año\")\n",
    "\n",
    "# Asignar el valor de adulto equivalente con valores por defecto\n",
    "def obtener_equivalente(row):\n",
    "    if (row['Edad'], 'Mujeres' if row['CH04'] == 2 else 'Varones') in equivalencias:\n",
    "        return equivalencias[(row['Edad'], 'Mujeres' if row['CH04'] == 2 else 'Varones')]\n",
    "    else:\n",
    "        return 1.0 if row['CH06'] > 18 else 0.5\n",
    "\n",
    "usu_individual['adulto_equiv'] = usu_individual.apply(obtener_equivalente, axis=1)\n",
    "\n",
    "# Sumar los valores de adulto_equiv para cada hogar\n",
    "ad_equiv_hogar = usu_individual.groupby(['CODUSU', 'NRO_HOGAR'])['adulto_equiv'].sum().reset_index()\n",
    "\n",
    "# Renombrar la columna\n",
    "ad_equiv_hogar.rename(columns={'adulto_equiv': 'ad_equiv_hogar'}, inplace=True)\n",
    "\n",
    "# Unir esta nueva columna con la tabla original `usu_individual`\n",
    "usu_individual = usu_individual.merge(ad_equiv_hogar, on=['CODUSU', 'NRO_HOGAR'], how='left')\n",
    "\n",
    "# Limpiar la duplicación de columnas si es necesario\n",
    "if 'ad_equiv_hogar_x' in usu_individual.columns and 'ad_equiv_hogar_y' in usu_individual.columns:\n",
    "    usu_individual = usu_individual.drop(columns=['ad_equiv_hogar_x']).rename(columns={'ad_equiv_hogar_y': 'ad_equiv_hogar'})\n",
    "\n",
    "print(usu_individual[['CODUSU', 'NRO_HOGAR', 'CH04', 'CH06', 'Edad', 'adulto_equiv', 'ad_equiv_hogar']].head(20))\n",
    "\n",
    "# Dividir los datos en respondieron y norespondieron según ITF_persona\n",
    "respondieron = usu_individual[usu_individual['ITF'] > 0].copy()\n",
    "norespondieron = usu_individual[usu_individual['ITF'] == 0].copy()\n",
    "\n",
    "# Guardar las bases distintas\n",
    "respondieron.to_csv('C:/Users/User/Downloads/Big data - TP2/TP3/respondieron.csv', index=False)\n",
    "norespondieron.to_csv('C:/Users/User/Downloads/Big data - TP2/TP3/norespondieron.csv', index=False)\n",
    "\n",
    "print(f\"Cantidad de personas que no respondieron su ITF: {len(norespondieron)}\")\n",
    "print(f\"Cantidad de personas que respondieron su ITF: {len(respondieron)}\")\n",
    "\n",
    "# Definir el valor de la Canasta Básica Total para un adulto equivalente\n",
    "canasta_basica_total = 132853.3\n",
    "\n",
    "# Calcular el ingreso necesario para cada hogar en respondieron\n",
    "respondieron['ingreso_necesario'] = respondieron['ad_equiv_hogar'] * canasta_basica_total\n",
    "\n",
    "# Calcular si la familia es pobre en respondieron\n",
    "respondieron['pobre'] = (respondieron['ITF'] < respondieron['ingreso_necesario']).astype(int)\n",
    "\n",
    "# Guardar la base actualizada\n",
    "respondieron.to_csv('C:/Users/User/Downloads/Big data - TP2/TP3/respondieron_actualizado.csv', index=False)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame resultante para verificar\n",
    "print(respondieron[['CODUSU', 'NRO_HOGAR', 'ITF', 'ad_equiv_hogar', 'ingreso_necesario', 'pobre']].head(20))\n",
    "\n",
    "# Mostrar el número total de hogares pobres\n",
    "total_hogares_pobres = respondieron[respondieron['pobre'] == 1].shape[0]\n",
    "total_hogares = respondieron.shape[0]\n",
    "\n",
    "print(f\"Total de hogares pobres: {total_hogares_pobres}\")\n",
    "print(f\"Total de hogares: {total_hogares}\")\n",
    "print(f\"Proporción de hogares pobres: {total_hogares_pobres / total_hogares:.2%}\")\n",
    "\n",
    "# Guardar la base norespondieron actualizada\n",
    "norespondieron.to_csv('C:/Users/User/Downloads/Big data - TP2/TP3/norespondieron_actualizado.csv', index=False)\n",
    "\n",
    "# Cargar la base de hogares para calcular la tasa de pobreza\n",
    "datos_hogar = pd.read_excel('C:/Users/User/Downloads/Big data - TP2/TP3/usu_hogar_T423.xlsx')\n",
    "\n",
    "# Filtrar datos para GBA (REGION == 1)\n",
    "gba_hogar = datos_hogar[datos_hogar['REGION'] == 1].copy()\n",
    "\n",
    "# Unir los datos de hogares con los datos de respondieron\n",
    "hogares_completos = pd.merge(gba_hogar, respondieron, on=['CODUSU', 'NRO_HOGAR'], how='left')\n",
    "\n",
    "# Calcular la tasa de pobreza ponderada utilizando PONDIH\n",
    "hogares_completos['es_pobre'] = hogares_completos['pobre'].fillna(0)\n",
    "\n",
    "# Calcular la tasa de pobreza ponderada\n",
    "total_hogares_ponderado = hogares_completos['PONDIH'].sum()\n",
    "total_hogares_pobres_ponderado = hogares_completos[hogares_completos['es_pobre'] == 1]['PONDIH'].sum()\n",
    "tasa_pobreza_ponderada = total_hogares_pobres_ponderado / total_hogares_ponderado\n",
    "\n",
    "print(f\"Tasa de pobreza ponderada: {tasa_pobreza_ponderada:.2%}\")\n",
    "\n",
    "# Guardar los resultados\n",
    "hogares_completos.to_csv('C:/Users/User/Downloads/Big data - TP2/TP3/hogares_completos.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc47b98c-3d34-497b-9fc7-570c5f3a2f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARTE 2\n",
    "#INCISO 1\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "def evalua_metodo(modelo, X_train, y_train, X_test, y_test):\n",
    "    # Ajustar el modelo con los datos de entrenamiento\n",
    "    modelo.fit(X_train, y_train)\n",
    "    \n",
    "    # Predecir con los datos de prueba\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    y_prob = modelo.predict_proba(X_test)[:, 1]  # Probabilidades de la clase positiva\n",
    "    \n",
    "    # Calcular la matriz de confusión\n",
    "    matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Calcular las curvas ROC y AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    \n",
    "    # Calcular el accuracy score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Graficar la matriz de confusión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Matriz de Confusión')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # Graficar la curva ROC\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_score:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Devolver las métricas evaluadas\n",
    "    metricas = {\n",
    "        'matriz_confusion': matriz_confusion,\n",
    "        'auc_score': auc_score,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    return metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "597c35e0-0f8c-4388-9f49-0f4d6fd0fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INCISO 2:\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "def cross_validation(modelo, k, X, y):\n",
    "    # Inicializar KFold con k particiones\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Inicializar listas para almacenar las métricas de cada partición\n",
    "    accuracies = []\n",
    "    aucs = []\n",
    "    \n",
    "    # Iterar sobre cada partición generada por KFold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Crear conjuntos de entrenamiento y prueba para la partición actual\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Evaluar el modelo con la partición actual usando evalua_metodo\n",
    "        metricas = evalua_metodo(modelo, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        # Almacenar las métricas de la partición actual\n",
    "        accuracies.append(metricas['accuracy'])\n",
    "        aucs.append(metricas['auc_score'])\n",
    "    \n",
    "    # Calcular las métricas promedio y desviación estándar\n",
    "    accuracy_mean = np.mean(accuracies)\n",
    "    accuracy_std = np.std(accuracies)\n",
    "    auc_mean = np.mean(aucs)\n",
    "    auc_std = np.std(aucs)\n",
    "    \n",
    "    # Devolver las métricas evaluadas\n",
    "    metricas_cv = {\n",
    "        'accuracy_mean': accuracy_mean,\n",
    "        'accuracy_std': accuracy_std,\n",
    "        'auc_mean': auc_mean,\n",
    "        'auc_std': auc_std\n",
    "    }\n",
    "    \n",
    "    return metricas_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de1a072-f06a-4af9-8576-16a93156723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INCISO 3:\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def cross_validation(modelo, k, X, y):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        modelo.fit(X_train, y_train)\n",
    "        y_pred = modelo.predict(X_test)\n",
    "        \n",
    "        error = mean_squared_error(y_test, y_pred)\n",
    "        errors.append(error)\n",
    "    \n",
    "    return np.mean(errors)\n",
    "\n",
    "def evalua_config(modelo, configs, X, y, k=5):\n",
    "    best_config = None\n",
    "    best_error = float('inf')\n",
    "    \n",
    "    for config in configs:\n",
    "        modelo.set_params(**config)\n",
    "        error = cross_validation(modelo, k, X, y)\n",
    "        \n",
    "        if error < best_error:\n",
    "            best_error = error\n",
    "            best_config = config\n",
    "    \n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d31c0d25-e51b-4399-af31-af6724dd723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INCISO 4:\n",
    "def evalua_metodo(modelo, X_train, y_train, X_test, y_test):\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    y_prob = modelo.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    metricas = {\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': auc_score\n",
    "    }\n",
    "    \n",
    "    return metricas\n",
    "\n",
    "def cross_validation(modelo, k, X, y):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        modelo.fit(X_train, y_train)\n",
    "        y_pred = modelo.predict(X_test)\n",
    "        \n",
    "        error = mean_squared_error(y_test, y_pred)\n",
    "        errors.append(error)\n",
    "    \n",
    "    return np.mean(errors)\n",
    "\n",
    "def evalua_config(modelo, configs, X, y, k=5):\n",
    "    best_config = None\n",
    "    best_error = float('inf')\n",
    "    \n",
    "    for config in configs:\n",
    "        modelo.set_params(**config)\n",
    "        error = cross_validation(modelo, k, X, y)\n",
    "        \n",
    "        if error < best_error:\n",
    "            best_error = error\n",
    "            best_config = config\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "def evalua_multiples_metodos(X, y, lambdas, k=5):\n",
    "    resultados = []\n",
    "\n",
    "    for lmbda in lambdas:\n",
    "        # Configuraciones para la regresión logística\n",
    "        configs_logistic = [\n",
    "            {'penalty': 'l1', 'C': lmbda, 'solver': 'liblinear'},\n",
    "            {'penalty': 'l2', 'C': lmbda}\n",
    "        ]\n",
    "        modelo_logistic = LogisticRegression()\n",
    "        best_config_logistic = evalua_config(modelo_logistic, configs_logistic, X, y, k)\n",
    "\n",
    "        if best_config_logistic is not None:\n",
    "            modelo_logistic.set_params(**best_config_logistic)\n",
    "            metricas_logistic = evalua_metodo(modelo_logistic, X_train, y_train, X_test, y_test)\n",
    "            resultados.append({\n",
    "                'model': 'Logistic Regression',\n",
    "                'config': best_config_logistic,\n",
    "                'lambda': lmbda,\n",
    "                'accuracy': metricas_logistic['accuracy'],\n",
    "                'roc_auc': metricas_logistic['roc_auc']\n",
    "            })\n",
    "        else:\n",
    "            print(\"No se encontró una configuración óptima para la regresión logística.\")\n",
    "            \n",
    "   # Modelo de Análisis de Discriminante Lineal\n",
    "    modelo_lda = LinearDiscriminantAnalysis()\n",
    "    metricas_lda = evalua_metodo(modelo_lda, X_train, y_train, X_test, y_test)\n",
    "    resultados.append({\n",
    "        'model': 'Linear Discriminant Analysis',\n",
    "        'config': 'Default',\n",
    "        'lambda': 'N/A',\n",
    "        'accuracy': metricas_lda['accuracy'],\n",
    "        'roc_auc': metricas_lda['roc_auc']\n",
    "    })\n",
    "    \n",
    "    # Modelo de K-Vecinos más Cercanos (K = 3)\n",
    "    modelo_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    metricas_knn = evalua_metodo(modelo_knn, X_train, y_train, X_test, y_test)\n",
    "    resultados.append({\n",
    "        'model': 'K-Nearest Neighbors',\n",
    "        'config': {'n_neighbors': 3},\n",
    "        'lambda': 'N/A',\n",
    "        'accuracy': metricas_knn['accuracy'],\n",
    "        'roc_auc': metricas_knn['roc_auc']\n",
    "    })\n",
    "    \n",
    "    # Crear un DataFrame con los resultados\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    \n",
    "    return df_resultados\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9eb3e40-5e8f-4d69-b320-9f97be779bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Respondieron DataFrame:\n",
      "                          CODUSU  ANO4  TRIMESTRE  NRO_HOGAR  COMPONENTE  H15  \\\n",
      "0  TQRMNOPUUHJKLQCDEIIAD00793187  2023          4          1           1    2   \n",
      "1  TQRMNOPUUHJKLQCDEIIAD00793187  2023          4          1           2    1   \n",
      "2  TQRMNOPRWHKMLMCDEIJAH00794278  2023          4          1           2    1   \n",
      "3  TQRMNOPRWHKMLMCDEIJAH00794278  2023          4          1           3    1   \n",
      "4  TQRMNOPRWHKMLMCDEIJAH00794278  2023          4          1           4    1   \n",
      "\n",
      "   REGION MAS_500  AGLOMERADO  PONDERA  ...  ADECIFR  IPCF DECCFR  IDECCFR  \\\n",
      "0       1       S          32      609  ...       12   0.0     12      NaN   \n",
      "1       1       S          32      609  ...       12   0.0     12      NaN   \n",
      "2       1       S          33     2254  ...       12   0.0     12      NaN   \n",
      "3       1       S          33     2254  ...       12   0.0     12      NaN   \n",
      "4       1       S          33     2254  ...       12   0.0     12      NaN   \n",
      "\n",
      "   RDECCFR  GDECCFR  PDECCFR  ADECCFR  PONDIH     Edad  \n",
      "0       12       12      NaN       12       0  65 años  \n",
      "1       12       12      NaN       12       0  66 años  \n",
      "2       12       12      NaN       12       0  24 años  \n",
      "3       12       12      NaN       12       0  26 años  \n",
      "4       12       12      NaN       12       0  27 años  \n",
      "\n",
      "[5 rows x 167 columns]\n",
      "\n",
      "Respondieron DataFrame:\n",
      "                          CODUSU  ANO4  TRIMESTRE  NRO_HOGAR  COMPONENTE  H15  \\\n",
      "0  TQRMNOTUUHJMLPCDEIIAD00801670  2023          4          1           1    1   \n",
      "1  TQRMNOTUUHJMLPCDEIIAD00801670  2023          4          1           2    1   \n",
      "2  TQRMNOPUTHKLMNCDEIIAD00791268  2023          4          1           1    1   \n",
      "3  TQRMNOPUTHKLMNCDEIIAD00791268  2023          4          1           2    1   \n",
      "4  TQRMNOPUTHKLMNCDEIIAD00791268  2023          4          1           3    1   \n",
      "\n",
      "   REGION MAS_500  AGLOMERADO  PONDERA  ...       IPCF  DECCFR IDECCFR  \\\n",
      "0       1       S          32     1775  ...  225000.00       8     NaN   \n",
      "1       1       S          32     1775  ...  225000.00       8     NaN   \n",
      "2       1       S          32     3129  ...  266666.67       9     NaN   \n",
      "3       1       S          32     3129  ...  266666.67       9     NaN   \n",
      "4       1       S          32     3129  ...  266666.67       9     NaN   \n",
      "\n",
      "   RDECCFR  GDECCFR  PDECCFR  ADECCFR  PONDIH     Edad  pobre  \n",
      "0        8        8      NaN        5    3079  76 años      0  \n",
      "1        8        8      NaN        5    3079  79 años      0  \n",
      "2        8        8      NaN        6    4648  49 años      0  \n",
      "3        8        8      NaN        6    4648  20 años      0  \n",
      "4        8        8      NaN        6    4648  17 años      0  \n",
      "\n",
      "[5 rows x 168 columns]\n"
     ]
    }
   ],
   "source": [
    "#PARTE 3:\n",
    "#INCISO 1:\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar los datasets con la opción low_memory=False\n",
    "no_respondieron_df = pd.read_csv('C:/Users/User/Downloads/Big data - TP2/TP3/norespondieron.csv', low_memory=False)\n",
    "respondieron_df = pd.read_csv('C:/Users/User/Downloads/Big data - TP2/TP3/respondieron_actualizado.csv', low_memory=False)\n",
    "\n",
    "# Columnas a eliminar en cada dataset\n",
    "columns_to_drop_no_respondieron = ['PP02C1', 'PP02C2', 'PP02C3', 'PP02C4', 'PP02C5', 'PP02C6', 'PP02C7', 'PP02C8', 'PP02E', 'PP02H', 'PP02I', 'adulto_equiv', 'ad_equiv_hogar']\n",
    "columns_to_drop_respondieron = ['PP02C1', 'PP02C2', 'PP02C3', 'PP02C4', 'PP02C5', 'PP02C6', 'PP02C7', 'PP02C8', 'PP02E', 'PP02H', 'PP02I', 'adulto_equiv', 'ad_equiv_hogar', 'ingreso_necesario']\n",
    "\n",
    "# Eliminar las columnas\n",
    "no_respondieron_df.drop(columns=columns_to_drop_no_respondieron, inplace=True)\n",
    "respondieron_df.drop(columns=columns_to_drop_respondieron, inplace=True)\n",
    "\n",
    "# Mostrar las primeras filas de los dataframes modificados\n",
    "print(\"No Respondieron DataFrame:\")\n",
    "print(no_respondieron_df.head())\n",
    "\n",
    "print(\"\\nRespondieron DataFrame:\")\n",
    "print(respondieron_df.head())\n",
    "\n",
    "# Guardar los dataframes modificados en nuevos archivos CSV\n",
    "no_respondieron_df.to_csv('C:/Users/User/Downloads/Big data - TP2/TP3/norespondieron_modified.csv', index=False)\n",
    "respondieron_df.to_csv('C:/Users/User/Downloads/Big data - TP2/TP3/respondieron_actualizado_modified.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b377085-0828-4018-95ba-4e4bb6b29651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas presentes en el DataFrame:\n",
      "Index(['CODUSU', 'ANO4', 'TRIMESTRE', 'NRO_HOGAR', 'COMPONENTE', 'H15',\n",
      "       'REGION', 'MAS_500', 'AGLOMERADO', 'PONDERA',\n",
      "       ...\n",
      "       'RDECCFR', 'GDECCFR', 'PDECCFR', 'ADECCFR', 'PONDIH', 'Edad',\n",
      "       'adulto_equiv', 'ad_equiv_hogar', 'ingreso_necesario', 'pobre'],\n",
      "      dtype='object', length=182)\n",
      "Shape of X: (4401, 179)\n",
      "Shape of y: (4401,)\n"
     ]
    }
   ],
   "source": [
    "#Continuación del INCISO 1:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el dataset \n",
    "df = pd.read_csv('C:/Users/User/Downloads/Big data - TP2/TP3/respondieron_actualizado.csv', low_memory=False)\n",
    "\n",
    "# Mostrar las columnas presentes en el DataFrame\n",
    "print(\"Columnas presentes en el DataFrame:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Definir la variable dependiente (vector y)\n",
    "y = df['pobre'].values\n",
    "\n",
    "# Columnas que se deben eliminar de la matriz de variables independientes\n",
    "columns_to_drop_from_X = ['pobre', 'CODUSU', 'MAS_500_x', 'MAS_500_y', 'CH05', 'PP09A_ESP']\n",
    "\n",
    "# Eliminar solo las columnas que existen en el DataFrame\n",
    "columns_to_drop = [col for col in columns_to_drop_from_X if col in df.columns]\n",
    "\n",
    "# Definir las variables independientes (matriz X) eliminando las columnas especificadas\n",
    "X = df.drop(columns=columns_to_drop).values\n",
    "\n",
    "# Agregar una columna de unos a la matriz X si es necesario (para el término de intercepto en un modelo lineal)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# Verificar las dimensiones de X e y\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d73309a2-1dd2-4598-b697-d36202d31056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay NaN en X: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          model                       config lambda  accuracy  \\\n",
      "0           Logistic Regression  {'penalty': 'l2', 'C': 0.2}    0.2  0.997730   \n",
      "1           Logistic Regression    {'penalty': 'l2', 'C': 2}      2  0.997730   \n",
      "2           Logistic Regression    {'penalty': 'l2', 'C': 8}      8  0.997730   \n",
      "3  Linear Discriminant Analysis                      Default    N/A  0.961407   \n",
      "4           K-Nearest Neighbors           {'n_neighbors': 3}    N/A  0.970488   \n",
      "\n",
      "    roc_auc  \n",
      "0  1.000000  \n",
      "1  1.000000  \n",
      "2  1.000000  \n",
      "3  0.993790  \n",
      "4  0.992707  \n"
     ]
    }
   ],
   "source": [
    "#INCISO 2:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv('C:/Users/User/Downloads/Big data - TP2/TP3/respondieron_actualizado.csv', low_memory=False)\n",
    "\n",
    "# Definir la variable dependiente (vector y)\n",
    "y = df['pobre'].values\n",
    "\n",
    "# Columnas que se deben eliminar de la matriz de variables independientes\n",
    "columns_to_drop_from_X = ['pobre', 'CODUSU', 'MAS_500_x', 'MAS_500_y', 'CH05', 'PP09A_ESP']\n",
    "\n",
    "# Eliminar solo las columnas que existen en el DataFrame\n",
    "columns_to_drop = [col for col in columns_to_drop_from_X if col in df.columns]\n",
    "\n",
    "# Definir las variables independientes (matriz X) eliminando las columnas especificadas\n",
    "X = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Asegurarse de que todas las columnas en X sean numéricas\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Usar SimpleImputer para reemplazar valores NaN con la media de la columna\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Verificar si quedan NaN en X\n",
    "print(\"Hay NaN en X:\", np.isnan(X).any())\n",
    "\n",
    "# Agregar una columna de unos a la matriz X si es necesario (para el término de intercepto en un modelo lineal)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def evalua_metodo(modelo, X_train, y_train, X_test, y_test):\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    y_prob = modelo.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    metricas = {\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': auc_score\n",
    "    }\n",
    "    \n",
    "    return metricas\n",
    "\n",
    "def cross_validation(modelo, k, X, y):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        modelo.fit(X_train, y_train)\n",
    "        y_pred = modelo.predict(X_test)\n",
    "        \n",
    "        error = mean_squared_error(y_test, y_pred)\n",
    "        errors.append(error)\n",
    "    \n",
    "    return np.mean(errors)\n",
    "\n",
    "def evalua_config(modelo, configs, X, y, k=5):\n",
    "    best_config = None\n",
    "    best_error = float('inf')\n",
    "    \n",
    "    for config in configs:\n",
    "        modelo.set_params(**config)\n",
    "        error = cross_validation(modelo, k, X, y)\n",
    "        \n",
    "        if error < best_error:\n",
    "            best_error = error\n",
    "            best_config = config\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "def evalua_multiples_metodos(X, y, lambdas, k=5):\n",
    "    resultados = []\n",
    "\n",
    "    for lmbda in lambdas:\n",
    "        # Configuraciones para la regresión logística\n",
    "        configs_logistic = [\n",
    "            {'penalty': 'l1', 'C': lmbda, 'solver': 'liblinear'},\n",
    "            {'penalty': 'l2', 'C': lmbda}\n",
    "        ]\n",
    "        modelo_logistic = LogisticRegression()\n",
    "        best_config_logistic = evalua_config(modelo_logistic, configs_logistic, X, y, k)\n",
    "\n",
    "        if best_config_logistic is not None:\n",
    "            modelo_logistic.set_params(**best_config_logistic)\n",
    "            metricas_logistic = evalua_metodo(modelo_logistic, X_train, y_train, X_test, y_test)\n",
    "            resultados.append({\n",
    "                'model': 'Logistic Regression',\n",
    "                'config': best_config_logistic,\n",
    "                'lambda': lmbda,\n",
    "                'accuracy': metricas_logistic['accuracy'],\n",
    "                'roc_auc': metricas_logistic['roc_auc']\n",
    "            })\n",
    "        else:\n",
    "            print(\"No se encontró una configuración óptima para la regresión logística.\")\n",
    "    \n",
    "    # Modelo de Análisis de Discriminante Lineal\n",
    "    modelo_lda = LinearDiscriminantAnalysis()\n",
    "    metricas_lda = evalua_metodo(modelo_lda, X_train, y_train, X_test, y_test)\n",
    "    resultados.append({\n",
    "        'model': 'Linear Discriminant Analysis',\n",
    "        'config': 'Default',\n",
    "        'lambda': 'N/A',\n",
    "        'accuracy': metricas_lda['accuracy'],\n",
    "        'roc_auc': metricas_lda['roc_auc']\n",
    "    })\n",
    "    \n",
    "    # Modelo de K-Vecinos más Cercanos (K = 3)\n",
    "    modelo_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    metricas_knn = evalua_metodo(modelo_knn, X_train, y_train, X_test, y_test)\n",
    "    resultados.append({\n",
    "        'model': 'K-Nearest Neighbors',\n",
    "        'config': {'n_neighbors': 3},\n",
    "        'lambda': 'N/A',\n",
    "        'accuracy': metricas_knn['accuracy'],\n",
    "        'roc_auc': metricas_knn['roc_auc']\n",
    "    })\n",
    "    \n",
    "    # Crear un DataFrame con los resultados\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    \n",
    "    return df_resultados\n",
    "\n",
    "# Ejecutar la función con la base respondieron\n",
    "lambdas = [0.2, 2, 8]\n",
    "resultados = evalua_multiples_metodos(X, y, lambdas, k=5)\n",
    "print(resultados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d890a-81c2-4f8b-bfbb-f5e6b28bcecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv('C:/Users/User/Downloads/Big data - TP2/TP3/respondieron_actualizado.csv', low_memory=False)\n",
    "\n",
    "# Definir la variable dependiente (vector y)\n",
    "y = df['pobre'].values\n",
    "\n",
    "# Columnas que se deben eliminar de la matriz de variables independientes\n",
    "columns_to_drop_from_X = ['pobre', 'CODUSU', 'MAS_500_x', 'MAS_500_y', 'CH05', 'PP09A_ESP']\n",
    "\n",
    "# Eliminar solo las columnas que existen en el DataFrame\n",
    "columns_to_drop = [col for col in columns_to_drop_from_X if col in df.columns]\n",
    "\n",
    "# Definir las variables independientes (matriz X) eliminando las columnas especificadas\n",
    "X = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Asegurarse de que todas las columnas en X sean numéricas\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Usar SimpleImputer para reemplazar valores NaN con la media de la columna\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Verificar si quedan NaN en X\n",
    "print(\"Hay NaN en X:\", np.isnan(X).any())\n",
    "\n",
    "# Definir la lista de valores de lambda a probar\n",
    "lambdas = [10**n for n in range(-5, 6)]\n",
    "\n",
    "# Crear modelos de regresión logística para Ridge y LASSO\n",
    "modelo_ridge = LogisticRegression(penalty='l2', solver='liblinear', max_iter=10000)\n",
    "modelo_lasso = LogisticRegression(penalty='l1', solver='liblinear', max_iter=10000)\n",
    "\n",
    "# Función para realizar validación cruzada y obtener el error\n",
    "def obtener_errores_cv(modelo, X, y, lambdas, k=10):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    errores_promedio = []\n",
    "    proporciones_ceros = []\n",
    "\n",
    "    for lmbda in lambdas:\n",
    "        modelo.set_params(C=lmbda)\n",
    "        errores = []\n",
    "        ceros = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            modelo.fit(X_train, y_train)\n",
    "            y_pred = modelo.predict(X_test)\n",
    "            error = mean_squared_error(y_test, y_pred)\n",
    "            errores.append(error)\n",
    "\n",
    "            if hasattr(modelo, 'coef_'):\n",
    "                ceros.append(np.mean(modelo.coef_ == 0))\n",
    "\n",
    "        errores_promedio.append(errores)\n",
    "        proporciones_ceros.append(ceros)\n",
    "\n",
    "    return errores_promedio, proporciones_ceros\n",
    "\n",
    "# Obtener errores de validación cruzada\n",
    "errores_ridge, _ = obtener_errores_cv(modelo_ridge, X, y, lambdas)\n",
    "errores_lasso, ceros_lasso = obtener_errores_cv(modelo_lasso, X, y, lambdas)\n",
    "\n",
    "# Convertir los errores en DataFrame para box-plots\n",
    "df_errores_ridge = pd.DataFrame(errores_ridge, index=[f\"10^{n}\" for n in range(-5, 6)]).T\n",
    "df_errores_lasso = pd.DataFrame(errores_lasso, index=[f\"10^{n}\" for n in range(-5, 6)]).T\n",
    "df_ceros_lasso = pd.DataFrame(ceros_lasso, index=[f\"10^{n}\" for n in range(-5, 6)]).T\n",
    "\n",
    "# Graficar los box-plots\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=df_errores_ridge)\n",
    "plt.title('Ridge Regression: Prediction Error Distribution by Lambda')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=df_errores_lasso)\n",
    "plt.title('LASSO Regression: Prediction Error Distribution by Lambda')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df_ceros_lasso)\n",
    "plt.title('LASSO Regression: Proportion of Zero Coefficients by Lambda')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Proportion of Zero Coefficients')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Guardar resultados\n",
    "df_errores_ridge.to_csv('ridge_errors.csv', index=False)\n",
    "df_errores_lasso.to_csv('lasso_errors.csv', index=False)\n",
    "df_ceros_lasso.to_csv('lasso_zeros.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f896e5c-0dfd-4571-ba2c-21755e19dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INCISO 6:\n",
    "# Cargar resultados de errores y proporción de ceros\n",
    "df_errores_lasso = pd.read_csv('lasso_errors.csv')\n",
    "df_ceros_lasso = pd.read_csv('lasso_zeros.csv')\n",
    "\n",
    "# Seleccionar el mejor lambda para LASSO basado en el menor error\n",
    "mean_errores_lasso = df_errores_lasso.mean()\n",
    "best_lambda_lasso = lambdas[mean_errores_lasso.argmin()]\n",
    "\n",
    "print(f\"Mejor lambda para LASSO: {best_lambda_lasso}\")\n",
    "\n",
    "# Identificar variables descartadas por LASSO con el mejor lambda\n",
    "modelo_lasso = LogisticRegression(penalty='l1', solver='liblinear', max_iter=10000, C=best_lambda_lasso)\n",
    "modelo_lasso.fit(X, y)\n",
    "variables_descartadas = np.where(modelo_lasso.coef_ == 0)[1]\n",
    "nombres_variables_descartadas = df.drop(columns=columns_to_drop).columns[variables_descartadas]\n",
    "\n",
    "print(\"Variables descartadas por LASSO con mejor lambda:\")\n",
    "print(nombres_variables_descartadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75092f-ae15-4c01-b862-a281f2e6669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INCISO 7:\n",
    "# Cargar resultados de errores\n",
    "df_errores_ridge = pd.read_csv('ridge_errors.csv')\n",
    "df_errores_lasso = pd.read_csv('lasso_errors.csv')\n",
    "\n",
    "# Seleccionar el mejor lambda para Ridge y LASSO basado en el menor error\n",
    "mean_errores_ridge = df_errores_ridge.mean()\n",
    "mean_errores_lasso = df_errores_lasso.mean()\n",
    "\n",
    "best_lambda_ridge = lambdas[mean_errores_ridge.argmin()]\n",
    "best_lambda_lasso = lambdas[mean_errores_lasso.argmin()]\n",
    "\n",
    "print(f\"Mejor lambda para Ridge: {best_lambda_ridge}\")\n",
    "print(f\"Mejor lambda para LASSO: {best_lambda_lasso}\")\n",
    "\n",
    "# Calcular el error cuadrático medio para los mejores modelos\n",
    "modelo_ridge = LogisticRegression(penalty='l2', solver='liblinear', max_iter=10000, C=best_lambda_ridge)\n",
    "modelo_lasso = LogisticRegression(penalty='l1', solver='liblinear', max_iter=10000, C=best_lambda_lasso)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "ridge_errors = []\n",
    "lasso_errors = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    modelo_ridge.fit(X_train, y_train)\n",
    "    y_pred_ridge = modelo_ridge.predict(X_test)\n",
    "    ridge_errors.append(mean_squared_error(y_test, y_pred_ridge))\n",
    "    \n",
    "    modelo_lasso.fit(X_train, y_train)\n",
    "    y_pred_lasso = modelo_lasso.predict(X_test)\n",
    "    lasso_errors.append(mean_squared_error(y_test, y_pred_lasso))\n",
    "\n",
    "print(f\"Error cuadrático medio (ECM) para Ridge: {np.mean(ridge_errors)}\")\n",
    "print(f\"Error cuadrático medio (ECM) para LASSO: {np.mean(lasso_errors)}\")\n",
    "\n",
    "# Comparación de métodos de regularización\n",
    "if np.mean(ridge_errors) < np.mean(lasso_errors):\n",
    "    print(\"El método de regularización Ridge funciona mejor que LASSO basado en ECM.\")\n",
    "else:\n",
    "    print(\"El método de regularización LASSO funciona mejor que Ridge basado en ECM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8dd335-e630-4590-a862-b18970c28280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INCISO 9:\n",
    "# Cargar el dataset modificado\n",
    "no_respondieron_df = pd.read_csv('C:/Users/User/Downloads/Big data - TP2/TP3/norespondieron_modified.csv', low_memory=False)\n",
    "\n",
    "# Preprocesamiento similar a la base `respondieron`\n",
    "X_norespondieron = no_respondieron_df.apply(pd.to_numeric, errors='coerce')\n",
    "X_norespondieron = imputer.transform(X_norespondieron)\n",
    "X_norespondieron = np.hstack((np.ones((X_norespondieron.shape[0], 1)), X_norespondieron))\n",
    "\n",
    "# Predecir la pobreza con el mejor modelo de LASSO\n",
    "y_pred_norespondieron = modelo_lasso.predict(X_norespondieron)\n",
    "\n",
    "# Proporción de hogares pobres en la submuestra norespondieron\n",
    "proporcion_pobres = np.mean(y_pred_norespondieron)\n",
    "print(f\"Proporción de hogares pobres en la submuestra norespondieron: {proporcion_pobres:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
